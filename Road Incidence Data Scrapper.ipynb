{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a46e7f62-374b-428a-a55b-9cf6f664138f",
   "metadata": {},
   "source": [
    "# The scrapper extracts the road incidence data for Sydney using the *Historical Traffic API | TfNSW Open Data Hub and Developer Portal* using the API\n",
    "[Link to the API](https://opendata.transport.nsw.gov.au/dataset/historical-traffic-api)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31d749d3-4bcb-4d33-8e02-bae13e4b5cd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pickle\n",
    "import os\n",
    "from pandas_profiling import ProfileReport\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7940fba-9e40-4e6f-a7e5-dddb409cf99d",
   "metadata": {},
   "source": [
    "**Below are the mandatory inputs required from the user**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1e4975fc-e9cd-425c-8ceb-1555944b4b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the start date, end date and name of suburb for creating directory\n",
    "#these are mandatory!\n",
    "start_date = \"2012-01-01\" #Default: 01-01-2012\n",
    "end_date = \"2022-09-30\" #Default: Today's Date\n",
    "suburb = 'Suburb'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b81436e-f466-45ca-a980-fad974d8679d",
   "metadata": {},
   "source": [
    "**There are four major functions defined that provide the final dataset:**\n",
    "\n",
    "1. scrapper()\n",
    "   > extracts the data from the API for said duration and location <br>\n",
    "   > writes the extracted data pickle and json file\n",
    "---\n",
    "2. process()\n",
    "   > reads the pickle file created by scrapper() <br>\n",
    "   > fetches all the required columns <br>\n",
    "   > writes the processed data in pickle and csv file\n",
    "---\n",
    "3. clean_data()\n",
    "   > keeps data only inside the bounding box coordinates for Sydney <br>\n",
    "   > uses regex to clean some columns\n",
    "---\n",
    "4. reduce()\n",
    "   > keeps only the latest record for each incidence <br>\n",
    "   > keeps only the unique incidences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bf978526-1451-4616-8911-e9eb3c95aa46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def scraper(start_date=\"2012-01-01\", end_date=datetime.date.today().strftime(format=\"%Y-%m-%d\"), suburb=\"Suburb\", types=['Crash', 'Breakdown', 'Accident']):\n",
    "    api_url = \"https://api.transport.nsw.gov.au/v1/traffic/historicaldata\"\n",
    "    api_headers = {'Content-Type': 'application/json', 'Authorization': 'apikey ZRCn7V9FANYIdOBKduUwmkbfNDO1FHukUW2Z'}\n",
    "\n",
    "    start = int(start_date[:4])\n",
    "    if end_date > datetime.date.today().strftime(format=\"%Y-%m-%d\"): end_date = datetime.date.today().strftime(\n",
    "        format=\"%Y-%m-%d\")\n",
    "    end = int(end_date[:4])\n",
    "\n",
    "    qtrs = []\n",
    "    tick = 0\n",
    "    for year in range(start, end + 1):\n",
    "        if year % 4 == 0 and (year % 400 == 0 or year % 100 != 0):\n",
    "            i = 29\n",
    "        else:\n",
    "            i = 28\n",
    "        for month, day in zip([1, 3, 5, 7, 9, 11], [i, 30, 30, 31, 31, 31]):\n",
    "            if f\"{year}-{month:02}-01\" <= start_date <= f\"{year}-{month + 1:02}-{day}\" and tick == 0:\n",
    "                tick = 1\n",
    "                if end_date <= f\"{year}-{month + 1:02}-{day}\":\n",
    "                    qtrs.append([start_date, end_date])\n",
    "                    break\n",
    "                else:\n",
    "                    qtrs.append([start_date, f\"{year}-{month + 1:02}-{day}\"])\n",
    "            elif not (\n",
    "                    f\"{year}-{month:02}-01\" <= start_date <= f\"{year}-{month + 1:02}-{day}\") and tick == 0:\n",
    "                continue\n",
    "            else:\n",
    "                if end_date <= f\"{year}-{month + 1:02}-{day}\":\n",
    "                    qtrs.append([f\"{year}-{month:02}-01\", end_date])\n",
    "                    break\n",
    "                else:\n",
    "                    qtrs.append([f\"{year}-{month:02}-01\", f\"{year}-{month + 1:02}-{day}\"])\n",
    "\n",
    "    body = []\n",
    "    for i in range(len(qtrs)):\n",
    "        body.append('{ \"showHistory\": true,\"created\":\"' + qtrs[i][0] + '\",\"end\":\"' + qtrs[i][\n",
    "            1] + '\",\"radius\": 52,\"latitude\": -33.81745,\"longitude\": 150.9068}')\n",
    "\n",
    "    incidents = []\n",
    "    for text in body:\n",
    "        try:\n",
    "            response = requests.post(url=api_url, headers=api_headers, data=text)\n",
    "            try:\n",
    "                results = response.json()['result']\n",
    "                count = 0\n",
    "                for result in results:\n",
    "                    if result[\"Hazards\"][\"features\"][\"properties\"][\"mainCategory\"] in types:\n",
    "                        incidents.append(result)\n",
    "                        count += 1\n",
    "                print(response)\n",
    "                print(text[23:63])\n",
    "                print(count, '\\n')\n",
    "            except KeyError:\n",
    "                continue\n",
    "        except:\n",
    "            continue\n",
    "            print('Failed')\n",
    "\n",
    "    if not os.path.exists(f\"{start_date}_{end_date}_{suburb}\"):\n",
    "        os.makedirs(f\"{start_date}_{end_date}_{suburb}\")\n",
    "\n",
    "    with open(f\"{start_date}_{end_date}_{suburb}/Raw.pkl\", 'wb') as f:\n",
    "        pickle.dump(incidents, f)\n",
    "\n",
    "    with open(f\"{start_date}_{end_date}_{suburb}/Raw.json\", 'w') as f:\n",
    "        json.dump(incidents, f)\n",
    "\n",
    "\n",
    "def fetch_values(dic, *keys):\n",
    "    for key in keys:\n",
    "        try:\n",
    "            dic = dic[key]\n",
    "        except KeyError:\n",
    "            return None\n",
    "    return dic\n",
    "\n",
    "\n",
    "def process(start_date=\"2012-01-01\", end_date=datetime.date.today().strftime(format=\"%Y-%m-%d\"), suburb=\"Suburb\"):\n",
    "    #if not start_date: start_date = \"2012-01-01\"\n",
    "    #if not end_date: end_date = datetime.date.today().strftime(format=\"%Y-%m-%d\")\n",
    "    #if not suburb: suburb = \"Suburb\"\n",
    "    with open(f\"{start_date}_{end_date}_{suburb}/Raw.pkl\", 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "    records = []\n",
    "    for incident in df:\n",
    "        dict_H = incident['Hazards']\n",
    "        dict_F = dict_H['features']\n",
    "        dict_P = dict_F['properties']\n",
    "\n",
    "        ID = fetch_values(dict_F, 'id')\n",
    "        mainCategory = fetch_values(dict_P, 'mainCategory')\n",
    "        longitude = fetch_values(dict_F, 'geometry', 'coordinates')[0]\n",
    "        latitude = fetch_values(dict_F, 'geometry', 'coordinates')[1]\n",
    "        starttime = datetime.datetime.fromtimestamp(fetch_values(dict_F, 'incidentActualStartDate') / 1000.0)\n",
    "        lastUpdated = datetime.datetime.fromtimestamp(fetch_values(dict_P, 'lastUpdated') / 1000.0)\n",
    "        attendingGroups = ','.join(fetch_values(dict_P, 'attendingGroups'))\n",
    "        displayName = fetch_values(dict_P, 'displayName')\n",
    "        isMajor = fetch_values(dict_P, 'isMajor')\n",
    "        diversions = fetch_values(dict_P, 'diversions')\n",
    "        adviceA = fetch_values(dict_P, 'adviceA')\n",
    "        adviceB = fetch_values(dict_P, 'adviceB')\n",
    "        otherAdvice = fetch_values(dict_P, 'otherAdvice')\n",
    "        isNewIncident = fetch_values(dict_P, 'isNewIncident')\n",
    "        ended = fetch_values(dict_P, 'ended')\n",
    "        subCategoryA = fetch_values(dict_P, 'subCategoryA')\n",
    "        subCategoryB = fetch_values(dict_P, 'subCategoryB')\n",
    "        duration = (lastUpdated - starttime).total_seconds() / 60\n",
    "        try:\n",
    "            roads = fetch_values(dict_P, 'roads')[0]\n",
    "            if fetch_values(roads, 'impactedLanes'):\n",
    "                impactedlanes = fetch_values(dict_P, 'roads')[0]['impactedLanes'][0]\n",
    "\n",
    "                if fetch_values(dict_P, 'periods'):\n",
    "                    try:\n",
    "                        closureType = fetch_values(dict_P, 'periods')[0]['closureType']\n",
    "                        direction = fetch_values(dict_P, 'periods')[0]['direction']\n",
    "                    except:\n",
    "                        closureType = fetch_values(impactedlanes, 'extent')\n",
    "                        direction = fetch_values(impactedlanes, 'affectedDirection')\n",
    "                else:\n",
    "                    closureType = fetch_values(impactedlanes, 'extent')\n",
    "                    direction = fetch_values(impactedlanes, 'affectedDirection')\n",
    "\n",
    "                closedLanes = fetch_values(impactedlanes, 'closedLanes')\n",
    "                numberOfLanes = fetch_values(impactedlanes, 'numberOfLanes')\n",
    "\n",
    "            mainStreet = fetch_values(roads, 'mainStreet')\n",
    "            Suburb = fetch_values(roads, 'suburb')\n",
    "            trafficVolume = fetch_values(roads, 'trafficVolume')\n",
    "\n",
    "            record = [ID, mainCategory, longitude, latitude, starttime, lastUpdated, duration, subCategoryA,\n",
    "                      subCategoryB, attendingGroups, displayName, isMajor, diversions, adviceA, adviceB,\n",
    "                      otherAdvice, closureType, direction, mainStreet, closedLanes, numberOfLanes, Suburb,\n",
    "                      trafficVolume, ended, isNewIncident]\n",
    "\n",
    "            records.append(record)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(incident['Hazards']['features']['id'])\n",
    "            #continue\n",
    "    Final_Data = pd.DataFrame.from_records(\n",
    "        records,\n",
    "        columns=['ID', 'Main Category', 'Longitude', 'Latitude', 'Start Time', 'Last Updated',\n",
    "                 'Duration', 'Sub Category A', 'Sub Category B', 'Attending Groups',\n",
    "                 'Display Name', 'Is Major', 'Diversions', 'Advice A', 'Advice B', 'Other Advice',\n",
    "                 'Closure Type', 'Direction', 'Main Street', 'Closed Lanes', 'Number of Lanes',\n",
    "                 'Suburb', 'Traffic Volume', 'Ended', 'Is New Incident'])\n",
    "    Final_Data.reset_index()\n",
    "    with open(f\"{start_date}_{end_date}_{suburb}/Processed.pkl\", 'wb') as f:\n",
    "        pickle.dump(Final_Data, f)\n",
    "        f.close()\n",
    "    Final_Data.to_csv(f\"{start_date}_{end_date}_{suburb}/Processed.csv\")\n",
    "\n",
    "\n",
    "def clean_data(start_date=\"2012-01-01\", end_date=datetime.date.today().strftime(format=\"%Y-%m-%d\"), suburb=\"Suburb\"):\n",
    "    #if not start_date: start_date = \"2012-01-01\"\n",
    "    #if not end_date: end_date = datetime.date.today().strftime(format=\"%Y-%m-%d\")\n",
    "    #if not suburb: suburb = \"Suburb\"\n",
    "    with open(f\"{start_date}_{end_date}_{suburb}/Processed.pkl\", 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "    df = df[(df.Longitude >= 150.4605) & (df.Longitude <= 151.3531) & (df.Latitude >= -34.1004) &\n",
    "            (df.Latitude <= -33.5345)]\n",
    "    df = df[df.Duration >= 1]\n",
    "    df['Display Name'] = df['Display Name'].str.lower()\n",
    "    df['Main Category'] = np.where(\n",
    "        ~df['Display Name'].str.contains('crash|breakdown|accident', na=False), 'Other', df['Main Category'])\n",
    "    df['Main Category'] = np.where(df['Display Name'].str.contains('accident', na=False), 'Crash',\n",
    "                                   df['Main Category'])\n",
    "    df['Main Category'] = np.where(df['Main Category'].str.contains('Accident', na=False), 'Crash',\n",
    "                                   df['Main Category'])\n",
    "\n",
    "    for column in ['Diversions', 'Other Advice']:\n",
    "        new = []\n",
    "        for i in df.index:\n",
    "            try:\n",
    "                df.loc[i, column] = re.sub(re.compile('<.*?>'), '', df.loc[i, column])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.replace(['', ' '], np.nan, inplace=True)\n",
    "\n",
    "    with open(f\"{start_date}_{end_date}_{suburb}/Cleaned.pkl\", 'wb') as f:\n",
    "        pickle.dump(df, f)\n",
    "        f.close()\n",
    "    df.to_csv(f\"{start_date}_{end_date}_{suburb}/Cleaned.csv\")\n",
    "    #return df\n",
    "\n",
    "\n",
    "def reduce(start_date=\"2012-01-01\", end_date=datetime.date.today().strftime(format=\"%Y-%m-%d\"),suburb=\"Suburb\"):\n",
    "    with open(f\"{start_date}_{end_date}_{suburb}/cleaned.pkl\", 'rb') as f:\n",
    "        df = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "    reduced = pd.DataFrame()\n",
    "    for i in df.ID.unique():\n",
    "        reduced = pd.concat([reduced, df[df['Last Updated'] == max(df[df.ID == i]['Last Updated'])]], ignore_index=True)\n",
    "\n",
    "    reduced.drop_duplicates(inplace=True)\n",
    "\n",
    "    with open(f\"{start_date}_{end_date}_{suburb}/Reduced.pkl\", 'wb') as f:\n",
    "        pickle.dump(reduced, f)\n",
    "        f.close()\n",
    "    reduced.to_csv(f\"{start_date}_{end_date}_{suburb}/Reduced.csv\")\n",
    "    #return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6354b7d5",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "scraper(start_date=start_date,end_date=end_date,suburb=suburb)\n",
    "process(start_date=start_date,end_date=end_date,suburb=suburb)\n",
    "clean_data(start_date=start_date,end_date=end_date,suburb=suburb)\n",
    "reduce(start_date=start_date,end_date=end_date,suburb=suburb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d009a5-0119-42ff-99b9-2c6928f44bab",
   "metadata": {},
   "source": [
    "The below command performs a quick exploratory data analysis on the data and writes an HTML report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db526759-2286-4820-8986-6407acc295ec",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "ProfileReport(df).to_file('Report.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
